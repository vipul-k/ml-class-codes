{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<module 'p2' from 'C:\\\\Users\\\\vipul\\\\OneDrive\\\\Documents\\\\MPCS 53111 Machine Learning\\\\HW4\\\\hw4\\\\p2.py'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "import time\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import p2 as nn  \n",
    "importlib.reload(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Brief overview of your tasks in the accompanying Python script.  Please also see the outline in the homework instructions.\n",
    "- You need to complete the neural network by fill in codes where there is a #todo prompt.\n",
    "- **The architecture of the neural network you are building is a vanilla version of that of Pytorch/Tensorflow consisting of nodes of *Operation Classes*. Each *Operation Class* has a forward method (to calculate the operation result stored in self.value) and a backward method (to calculate the gradient of the operation w.r.t. the final loss function).**\n",
    "- You need to implement forward and backward methods for *Operation Classes* Mul, VDot, Sigmoid, Relu (not required), Softmax, Log. Add, Aref, and Accuracy has been implemented for you for reference\n",
    "- self.params is the list to store the trainable parameters (objects of Class Param). set_weights(weights) has been implemented for you where the provided weights and biases are converted to Param objects and stored in self.params. You need to study the code and implement <code>get_weights()</code> and <code>init_weights_with_xavier()</code>\n",
    "- self.components is the list to natively mimic the function of the computational graph. Helper functions nn_unary_op(op, x) and nn_binary_op(op, x, y) are provided to facilitate creating an operation and adding it to the computational graph. For example, instead of $a=b+c$, you should use <code>a = self.nn_binary_op(Add, Value(b), Value(c))</code>. Only in this way you can create an operation object <code>a</code> with <code>a.value</code> and <code>a.grad</code> to support the forward and backward method of the neural network.\n",
    "- Placeholder including <code>self.sample_placeholder</code>, <code>self.label_placeholder</code>, <code>self.pred_placeholder</code>, <code>self.loss_placeholder</code>, <code>self.accy_placeholder</code> are all empty vectors that will be assigned values when executing <code>forward</code> or <code>backward</code>. They facilitate the construcation of the computational graph. <code>self.sample_placeholder</code> is the input to the NN. We feed different examples by calling <code>self.sample_placeholder.set(X[i])</code> and <code>self.label_placeholder.set(y[i])</code>. <code>self.pred_placeholder</code>, <code>self.loss_placeholder</code>, <code>self.accy_placeholder</code> changes values in each iteration in <code>fit</code>.\n",
    "- <code>self.forward()</code> is provided for you where each operation object in self.components are evalued from the begining to the end.\n",
    "- <code>self.backward()</code> is provided for you where derivative of each operation object in self.components are calculated from the end (loss function) to the beginning.\n",
    "- You need to implement sgd_update_parameter.\n",
    "- You could implement gradient_estimate to debug.\n",
    "- A test function test_set_and_get_weights() has been provided for you to test <code>self.get_weights</code>. Feel free to create more test functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Below is the test code for get_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed the test for set_weights and get_weights.\n"
     ]
    }
   ],
   "source": [
    "# You should expect the following message\n",
    "nn.test_set_and_get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Below are test cases to help you debug the different operation Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "m1 = nn.InputValue(np.arange(12).reshape((3,4)))\n",
    "m2 = nn.InputValue(np.arange(12,24).reshape((3,4)))\n",
    "m3 = nn.InputValue(np.arange(24,36).reshape((3,4)))\n",
    "m4 = nn.InputValue(np.arange(36,48).reshape((3,4)))\n",
    "\n",
    "v1 = nn.InputValue(np.arange(3).reshape((3,)))\n",
    "v2 = nn.InputValue(np.arange(3,6).reshape((3,)))\n",
    "v3 = nn.InputValue(np.arange(6,9).reshape((3,)))\n",
    "v4 = nn.InputValue(np.arange(4).reshape((4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on VDot\n"
     ]
    }
   ],
   "source": [
    "#Test VDot\n",
    "x = nn.Mul(m1, m2)\n",
    "y = nn.VDot(v1, x)\n",
    "z = nn.Mul(y, v4)\n",
    "x.forward()\n",
    "y.forward()\n",
    "z.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "y.grad = 0\n",
    "x.grad = 0\n",
    "v1.grad = 0\n",
    "z.backward()\n",
    "y.backward()\n",
    "x.backward()\n",
    "\n",
    "yvalue = np.array([384., 463., 548., 639.])\n",
    "ygrad = np.array([0., 1., 2., 3.])\n",
    "yagrad = np.array([ 204.,  700., 1388.])\n",
    "ybgrad = np.array([[0., 0., 0., 0.],\n",
    "                   [0., 1., 2., 3.],\n",
    "                   [0., 2., 4., 6.]])\n",
    "\n",
    "if not np.array_equal(y.value, yvalue):\n",
    "    raise FailTestError(\"y.value not equal to matrix product of x.value and v1.value\")\n",
    "if not np.array_equal(y.grad, ygrad):\n",
    "    raise FailTestError(\"gradient of y is incorrect\")\n",
    "if not np.array_equal(y.a.grad, yagrad):\n",
    "    raise FailTestError(\"gradient of a in y is incorrect\")\n",
    "if not np.array_equal(y.b.grad, ybgrad):\n",
    "    raise FailTestError(\"gradient of b in y is incorrect\")\n",
    "print(\"Passed Test on VDot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on Sigmoid\n"
     ]
    }
   ],
   "source": [
    "# Test Sigmoid\n",
    "x = nn.Add(v1, v2)\n",
    "y = nn.Sigmoid(x)\n",
    "z = nn.Mul(y, v3)\n",
    "x.forward()\n",
    "y.forward()\n",
    "z.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "y.grad = 0\n",
    "x.grad = 0\n",
    "z.backward()\n",
    "y.backward()\n",
    "x.backward()\n",
    "\n",
    "yvalue = np.array([0.95257413, 0.9933072 , 0.999089], dtype=np.float32)\n",
    "yagrad = np.array([0.2710599 , 0.04653623, 0.00728134], dtype=np.float32)\n",
    "\n",
    "if not np.array_equal(np.round(y.value, 5), np.round(yvalue, 5)):\n",
    "    raise FailTestError(\"y.value not equal to sigmoid of x\")\n",
    "if not np.array_equal(np.round(y.a.grad, 5), np.round(yagrad, 5)):\n",
    "    raise FailTestError(\"gradient of a in y is incorrect\")\n",
    "print(\"Passed Test on Sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing this teset is optional.\n",
      "Passed Test on RELU\n"
     ]
    }
   ],
   "source": [
    "# Test RELU\n",
    "# THIS IS OPTIONAL.\n",
    "x = nn.Add(v1, v2)\n",
    "y = nn.RELU(x)\n",
    "z = nn.Mul(y, v3)\n",
    "x.forward()\n",
    "y.forward()\n",
    "z.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "y.grad = 0\n",
    "x.grad = 0\n",
    "z.backward()\n",
    "y.backward()\n",
    "x.backward()\n",
    "\n",
    "yvalue = np.array([3., 5., 7.], dtype=np.float32)\n",
    "yagrad = np.array([6., 7., 8.], dtype=np.float32)\n",
    "\n",
    "print(\"Passing this teset is optional.\")\n",
    "\n",
    "if not np.array_equal(np.round(y.value, 5), np.round(yvalue, 5)):\n",
    "    raise FailTestError(\"y.value not equal to Relu of x\")\n",
    "if not np.array_equal(np.round(y.a.grad, 5), np.round(yagrad, 5)):\n",
    "    raise FailTestError(\"gradient of a in y is incorrect\")\n",
    "print(\"Passed Test on RELU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on SoftMax\n"
     ]
    }
   ],
   "source": [
    "# Test SoftMax\n",
    "x = nn.Add(v1, v2)\n",
    "y = nn.SoftMax(x)\n",
    "z = nn.Mul(y, v3)\n",
    "x.forward()\n",
    "y.forward()\n",
    "z.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "y.grad = 0\n",
    "x.grad = 0\n",
    "z.backward()\n",
    "y.backward()\n",
    "x.backward()\n",
    "\n",
    "yvalue = np.array([0.01587624, 0.11731043, 0.86681336], dtype=np.float32)\n",
    "yagrad = np.array([-0.02938593, -0.09982383,  0.12920949], dtype=np.float32)\n",
    "\n",
    "if not np.array_equal(np.round(y.value, 5), np.round(yvalue, 5)):\n",
    "    raise FailTestError(\"y.value not equal to SoftMax of x\")\n",
    "if not np.array_equal(np.round(y.a.grad, 5), np.round(yagrad, 5)):\n",
    "    raise FailTestError(\"gradient of a in y is incorrect\")\n",
    "print(\"Passed Test on SoftMax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed Test on Log\n"
     ]
    }
   ],
   "source": [
    "# Test Log\n",
    "x = nn.Add(v1, v2)\n",
    "y = nn.Log(x)\n",
    "z = nn.Mul(y, v3)\n",
    "x.forward()\n",
    "y.forward()\n",
    "z.forward()\n",
    "z.grad = np.ones_like(z.value)\n",
    "y.grad = 0\n",
    "x.grad = 0\n",
    "z.backward()\n",
    "y.backward()\n",
    "x.backward()\n",
    "\n",
    "yvalue = np.array([1.0986123, 1.609438 , 1.9459102], dtype=np.float32)\n",
    "yagrad = np.array([2.       , 1.4      , 1.1428572], dtype=np.float32)\n",
    "\n",
    "if not np.array_equal(np.round(y.value, 5), np.round(yvalue, 5)):\n",
    "    raise FailTestError(\"y.value not equal to log of x\")\n",
    "if not np.array_equal(np.round(y.a.grad, 5), np.round(yagrad, 5)):\n",
    "    raise FailTestError(\"gradient of a in y is incorrect\")\n",
    "print(\"Passed Test on Log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Applying to the MNIST dataset\n",
    "- You should use the MNIST dataset to test the neural network you build above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# we will use train.csv for training and testing as it test.csv doesn't contain label\n",
    "data = pd.read_csv(\"./data/train.csv\")\n",
    "train_data = data.iloc[:30000]  # 30000\n",
    "test_data = data.iloc[30000:]  # 12000\n",
    "\n",
    "pixel_columns = [f\"pixel{i}\" for i in range(784)]\n",
    "\n",
    "# normalize by dividing by 255 as the pixel ranges from 0 to 255\n",
    "train_x = train_data[pixel_columns].values.astype(nn.DATA_TYPE)/255\n",
    "train_y = train_data[\"label\"].values.astype(nn.DATA_TYPE)\n",
    "\n",
    "test_x = test_data[pixel_columns].values.astype(nn.DATA_TYPE)/255\n",
    "test_y = test_data[\"label\"].values.astype(nn.DATA_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Debugging the fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'p2.Param'> <class 'p2.Param'>\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <Sigmoid> to the computational graph\n",
      "<class 'p2.Param'> <class 'p2.Param'>\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <SoftMax> to the computational graph\n",
      "Append <Aref> to the computational graph\n",
      "Append <Log> to the computational graph\n",
      "Append <Mul> to the computational graph\n",
      "Append <Accuracy> to the computational graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vipul\\OneDrive\\Documents\\MPCS 53111 Machine Learning\\HW4\\hw4\\p2.py:351: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  weights = np.array(weights)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0629aedc828d4b489336f3e4ea5be58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 2.4059, accy = 0.5000, [0.024 secs]\n",
      "Congrats! You have passed the test of your fit function, your NN model should be good to go!\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(nn)  # important line so that the changes you made on p2.py will be reflected without restarting the kernel\n",
    "# Here we use the 2-layer NN with 1 hidden layer, feel free to experiment on your own\n",
    "nodes_array = [784, 128, 10]\n",
    "model = nn.NN(nodes_array, \"sigmoid\")\n",
    "\n",
    "class FailTestError(Exception):\n",
    "    'Raised when a test fails'\n",
    "    pass\n",
    "\n",
    "# You can use the provided sample weights for initialization to help debug\n",
    "with open(\"./data/sample_weights.pkl\", 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "model.set_weights(weights)\n",
    "\n",
    "# You can use the first 2 samples to test if the gradients are correct\n",
    "X = train_x[:2]\n",
    "y = train_y[:2]\n",
    "\n",
    "# when calling fit, a computational graph will be built first, you should expect the exact lines printed\n",
    "model.fit(X, y, alpha=0.01, t=1)\n",
    "\n",
    "# Load the sample gradient for debugging\n",
    "with open(\"./data/sample_gradient.pkl\", 'rb') as f:\n",
    "    sample_grad = pickle.load(f)\n",
    "\n",
    "# first layer's weight of shape (784, 128)\n",
    "if not np.allclose(model.params[0].grad, sample_grad[\"w1\"]):\n",
    "    raise FailTestError(\"gradient of the first layer's weight is incorrect\")\n",
    "# first layer's bias of shape (128, )\n",
    "if not np.allclose(model.params[1].grad, sample_grad[\"b1\"]):\n",
    "    raise FailTestError(\"gradient of the first layer's bias is incorrect\")\n",
    "# second layer's weight of shape (128, 10)\n",
    "if not np.allclose(model.params[2].grad, sample_grad[\"w2\"]):\n",
    "    raise FailTestError(\"gradient of the second layer's weight is incorrect\")\n",
    "# second layer's bias of shape (10, )\n",
    "if not np.allclose(model.params[3].grad, sample_grad[\"b2\"]):\n",
    "    raise FailTestError(\"gradient of the second layer's bias is incorrect\")\n",
    "print(\"Congrats! You have passed the test of your fit function, your NN model should be good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Now train your NN on the whole training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {},
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'p2.Param'> <class 'p2.Param'>\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <Sigmoid> to the computational graph\n",
      "<class 'p2.Param'> <class 'p2.Param'>\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <SoftMax> to the computational graph\n",
      "Append <Aref> to the computational graph\n",
      "Append <Log> to the computational graph\n",
      "Append <Mul> to the computational graph\n",
      "Append <Accuracy> to the computational graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a89c8f1eb1e4979a19efc632c7170d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.3100, accy = 0.9109, [72.511 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9dbae409a604da9aa8fe047201b682d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.2371, accy = 0.9319, [72.628 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e41588ce7b548b09037d04bcdd936f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss = 0.1923, accy = 0.9446, [72.447 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc66434d4bc4196bf112d2e8de1a0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss = 0.1608, accy = 0.9536, [72.458 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bafa7b44f04ec898b596e01a88efa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss = 0.1371, accy = 0.9608, [71.957 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fd5624576a4fe796973b7b6341546d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss = 0.1187, accy = 0.9663, [71.682 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec939547dd8a45cb9e8bb2af441663f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss = 0.1040, accy = 0.9705, [71.324 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901a72815ed94ec5b45e827ad230f776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train loss = 0.0919, accy = 0.9740, [72.033 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937b29615dd54a5dbac3cf8f15f8b037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss = 0.0818, accy = 0.9776, [71.493 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ab74d2dcf144abbf98b58f1a130d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss = 0.0733, accy = 0.9800, [71.549 secs]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(nn)  # important line so that the changes you made on p2.py will be reflected without restarting the kernel\n",
    "# Here we use the 2-layer NN with 1 hidden layer, feel free to experiment on your own\n",
    "nodes_array = [784, 128, 10]\n",
    "model = nn.NN(nodes_array, \"sigmoid\")\n",
    "model.init_weights_with_xavier()\n",
    "model.fit(train_x, train_y, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.9637, accy = 0.1166\n"
     ]
    }
   ],
   "source": [
    "# After 10 epochs of training, you should expect an accuracy over 95% and loss around 0.1\n",
    "accy, loss = model.eval(test_x, test_y)\n",
    "print(\"Test accuracy = %.4f, accy = %.4f\" % (accy, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'p2.Param'> <class 'p2.Param'>\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <RELU> to the computational graph\n",
      "<class 'p2.Param'> <class 'p2.Param'>\n",
      "Append <VDot> to the computational graph\n",
      "Append <Add> to the computational graph\n",
      "Append <SoftMax> to the computational graph\n",
      "Append <Aref> to the computational graph\n",
      "Append <Log> to the computational graph\n",
      "Append <Mul> to the computational graph\n",
      "Append <Accuracy> to the computational graph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43d07d1239a467eb0786d101e7a9b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.1574, accy = 0.9525, [87.602 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0243137ae47a48c6be1d0b6613ab285d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.1008, accy = 0.9687, [86.883 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6e9a0d03f649eabd7c5df0eb39e755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss = 0.0655, accy = 0.9798, [85.609 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c3e3a137254491b275453409566602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss = 0.0480, accy = 0.9851, [87.162 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edccff94bb6045cdac54ea71ba4ed424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss = 0.0327, accy = 0.9906, [98.486 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dea971bda5742089b927c9818d66a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss = 0.0236, accy = 0.9934, [128.184 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9037ca7eee0d42c9b3a6182bca832d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss = 0.0201, accy = 0.9944, [95.168 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbf3ba191eb427fbe32c94063c251c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train loss = 0.0167, accy = 0.9955, [87.063 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e0cb058d3f4ee7807e3af2f9f22a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss = 0.0142, accy = 0.9963, [87.171 secs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe1dfe9ee324aa0a5dae48247cdaf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss = 0.0116, accy = 0.9971, [87.381 secs]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(nn)  # important line so that the changes you made on p2.py will be reflected without restarting the kernel\n",
    "# Here we use the 2-layer NN with 1 hidden layer, feel free to experiment on your own\n",
    "nodes_array = [784, 128, 10]\n",
    "model = nn.NN(nodes_array, \"relu\")\n",
    "model.init_weights_with_xavier()\n",
    "model.fit(train_x, train_y, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.9735, accy = 0.1052\n"
     ]
    }
   ],
   "source": [
    "# After 10 epochs of training, you should expect an accuracy over 95% and loss around 0.1\n",
    "accy, loss = model.eval(test_x, test_y)\n",
    "print(\"Test accuracy = %.4f, accy = %.4f\" % (accy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
